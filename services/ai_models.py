# services/ai_models.py
"""
Handles AI/ML model integration for signal filtering and market regime detection.
Includes functions for model training (placeholders), prediction, and feature engineering.
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# import xgboost as xgb # Uncomment when XGBoost is fully implemented

from utils.logger import get_logger
from config import settings

logger = get_logger(__name__)

class AIModelService:
    """
    A service class to encapsulate AI/ML model functionalities.
    """
    def __init__(self):
        self.signal_filter_model = None
        self.regime_detection_model = None
        self.feature_scaler = StandardScaler() # For scaling features before model prediction

    def _prepare_features_for_signal_filtering(self, price_data: pd.DataFrame, signals: pd.DataFrame) -> pd.DataFrame:
        """
        Prepares features from price data and raw signals for the signal filtering model.
        This is a crucial step and will likely need extensive customization based on strategy and data.

        Args:
            price_data (pd.DataFrame): OHLCV data, indexed by timestamp.
            signals (pd.DataFrame): Raw signals generated by a core strategy.
                                     Expected columns: 'SignalTime', 'EntryPrice', 'SignalType', etc.

        Returns:
            pd.DataFrame: DataFrame of features, indexed consistently with signals.
        """
        if signals.empty or price_data.empty:
            logger.warning("AIModelService: Empty signals or price data for feature preparation.")
            return pd.DataFrame()

        # Ensure signals index is DatetimeIndex and price_data is accessible
        if not isinstance(signals.index, pd.DatetimeIndex):
            try:
                signals = signals.set_index('SignalTime')
            except KeyError:
                logger.error("AIModelService: 'SignalTime' not found in signals DataFrame for indexing.")
                return pd.DataFrame()
        
        features_list = []

        # Example features (to be expanded significantly):
        # - Volatility (e.g., ATR over a period)
        # - Momentum (e.g., RSI, MACD)
        # - Price relative to moving averages
        # - Time-based features (hour of day, day of week)
        # - Signal characteristics (e.g., distance from entry to recent swing high/low)

        # For simplicity, let's use some basic features based on the signal bar
        # This needs to be aligned with how the model was trained.
        for signal_time, signal_row in signals.iterrows():
            try:
                # Find the corresponding bar in price_data
                # Using asof for exact or nearest preceding match
                bar_data_at_signal = price_data.loc[price_data.index.asof(signal_time)]
                
                if pd.isna(bar_data_at_signal).any(): # Check if bar_data_at_signal contains NaNs
                    logger.debug(f"AIModelService: NaN data for signal at {signal_time}. Skipping feature gen for this signal.")
                    features_list.append({'SignalTime': signal_time}) # Add placeholder to maintain index
                    continue

                feature_dict = {'SignalTime': signal_time}
                
                # Example: Price action features from the signal bar
                feature_dict['bar_range'] = bar_data_at_signal['High'] - bar_data_at_signal['Low']
                feature_dict['close_position_in_range'] = (bar_data_at_signal['Close'] - bar_data_at_signal['Low']) / feature_dict['bar_range'] if feature_dict['bar_range'] > 0 else 0.5
                
                # Example: Simple moving average (requires pre-calculation on price_data or on-the-fly calculation)
                # This is illustrative; in practice, indicators should be pre-calculated or efficiently accessed.
                # sma_period = 20
                # if len(price_data.loc[:signal_time]) >= sma_period:
                #     sma = price_data['Close'].loc[:signal_time].rolling(window=sma_period).mean().iloc[-1]
                #     feature_dict['price_vs_sma'] = bar_data_at_signal['Close'] - sma
                # else:
                #     feature_dict['price_vs_sma'] = 0

                # Add more sophisticated features here
                # e.g., ATR, RSI, MACD values at the time of the signal

                features_list.append(feature_dict)

            except KeyError:
                logger.warning(f"AIModelService: Could not find price data for signal at {signal_time}. Skipping feature gen.")
                features_list.append({'SignalTime': signal_time}) # Maintain index
            except Exception as e:
                logger.error(f"AIModelService: Error preparing features for signal at {signal_time}: {e}", exc_info=True)
                features_list.append({'SignalTime': signal_time})

        if not features_list:
            return pd.DataFrame()

        features_df = pd.DataFrame(features_list)
        if 'SignalTime' in features_df.columns:
             features_df = features_df.set_index('SignalTime')
        
        # Drop rows where essential features might be all NaN if placeholders were added
        features_df.dropna(how='all', inplace=True) 
        
        # Fill any remaining NaNs (e.g., if some features couldn't be calculated for early data points)
        # This strategy (e.g., ffill or mean imputation) depends on the feature and model.
        features_df.fillna(method='ffill', inplace=True) # Forward fill
        features_df.fillna(0, inplace=True) # Fill remaining NaNs with 0 (use with caution)

        return features_df

    def train_signal_filter_model(self, features: pd.DataFrame, labels: pd.Series, model_type: str = "RandomForest"):
        """
        Placeholder for training a signal filtering model.
        In a real scenario, this would involve loading historical data, generating signals,
        backtesting them to create labels (e.g., was the trade profitable?),
        then training the model.

        Args:
            features (pd.DataFrame): DataFrame of features.
            labels (pd.Series): Series of labels (e.g., 1 for good signal, 0 for bad).
            model_type (str): "RandomForest" or "XGBoost".
        """
        logger.info(f"AIModelService: Attempting to train signal filter model ({model_type})...")
        if features.empty or labels.empty:
            logger.error("AIModelService: Features or labels are empty. Cannot train model.")
            return

        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)
        
        # Scale features
        X_train_scaled = self.feature_scaler.fit_transform(X_train)
        # X_test_scaled = self.feature_scaler.transform(X_test) # Not used in this placeholder

        if model_type == "RandomForest":
            self.signal_filter_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
            self.signal_filter_model.fit(X_train_scaled, y_train)
            logger.info("AIModelService: RandomForest signal filter model trained (placeholder).")
        # elif model_type == "XGBoost":
            # self.signal_filter_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)
            # self.signal_filter_model.fit(X_train_scaled, y_train)
            # logger.info("AIModelService: XGBoost signal filter model trained (placeholder).")
        else:
            logger.error(f"AIModelService: Unsupported model type for signal filtering: {model_type}")
            return
        
        # In a real application, you'd save the trained model and scaler
        # import joblib
        # joblib.dump(self.signal_filter_model, f'signal_filter_{model_type.lower()}.pkl')
        # joblib.dump(self.feature_scaler, f'signal_filter_scaler_{model_type.lower()}.pkl')

    def filter_signals(self, price_data: pd.DataFrame, raw_signals: pd.DataFrame) -> pd.DataFrame:
        """
        Filters raw trading signals using the trained AI model.

        Args:
            price_data (pd.DataFrame): OHLCV data.
            raw_signals (pd.DataFrame): DataFrame of raw signals from a strategy.

        Returns:
            pd.DataFrame: DataFrame of filtered signals.
        """
        if self.signal_filter_model is None:
            logger.warning("AIModelService: Signal filter model not trained/loaded. Returning raw signals.")
            return raw_signals
        if raw_signals.empty:
            logger.info("AIModelService: No raw signals to filter.")
            return pd.DataFrame()

        logger.info(f"AIModelService: Filtering {len(raw_signals)} raw signals...")
        
        features_for_prediction = self._prepare_features_for_signal_filtering(price_data, raw_signals.copy())
        
        if features_for_prediction.empty:
            logger.warning("AIModelService: No features could be prepared for signal filtering. Returning raw signals.")
            return raw_signals
        
        # Ensure order of columns in features_for_prediction matches training
        # This is critical. In a robust system, feature names/order would be stored and enforced.
        # For this placeholder, we assume self.feature_scaler.transform will handle it if columns match.
        # However, if columns are missing or in different order, it will fail.
        # A more robust approach:
        # trained_feature_names = self.feature_scaler.feature_names_in_ (if available, or store them)
        # features_for_prediction = features_for_prediction[trained_feature_names]

        try:
            features_scaled = self.feature_scaler.transform(features_for_prediction)
        except ValueError as ve:
            logger.error(f"AIModelService: Error scaling features for prediction. This might be due to mismatch in feature columns/order or new NaNs. Error: {ve}", exc_info=True)
            logger.error(f"AIModelService: Scaler expected features: {getattr(self.feature_scaler, 'feature_names_in_', 'N/A')}")
            logger.error(f"AIModelService: Features provided for prediction: {features_for_prediction.columns.tolist()}")
            logger.error(f"AIModelService: Features head:\n{features_for_prediction.head()}")
            return raw_signals # Return raw signals if scaling fails

        predictions = self.signal_filter_model.predict(features_scaled) # predict returns 0 or 1
        
        # Filter signals based on predictions
        # Ensure raw_signals is indexed by SignalTime if features_for_prediction is
        if not isinstance(raw_signals.index, pd.DatetimeIndex) and 'SignalTime' in raw_signals.columns:
            raw_signals_indexed = raw_signals.set_index('SignalTime')
        else:
            raw_signals_indexed = raw_signals
        
        # Align predictions with raw_signals. Predictions are an array, features_for_prediction has the index.
        if len(predictions) != len(features_for_prediction.index):
            logger.error("AIModelService: Mismatch between number of predictions and number of feature rows. Cannot reliably filter.")
            return raw_signals

        # Create a Series from predictions with the same index as features_for_prediction
        predictions_series = pd.Series(predictions, index=features_for_prediction.index, name='AI_Filter_Pass')

        # Join this series with the original signals
        # Ensure raw_signals_indexed has a compatible index
        # If raw_signals_indexed still has a default RangeIndex, this join will fail.
        # It's safer to filter raw_signals based on the index of features_for_prediction where prediction is 1.
        
        passing_signal_indices = features_for_prediction.index[predictions == 1]
        
        # Filter the original raw_signals DataFrame
        # This requires raw_signals to be indexed by SignalTime or have a SignalTime column that matches.
        if isinstance(raw_signals_indexed.index, pd.DatetimeIndex):
            filtered_signals = raw_signals_indexed.loc[raw_signals_indexed.index.isin(passing_signal_indices)].copy()
        else: # Fallback if indexing is tricky, less efficient
            logger.warning("AIModelService: Raw signals not properly indexed by SignalTime for efficient filtering. Attempting merge.")
            temp_predictions_df = predictions_series.reset_index()
            # Assuming raw_signals has 'SignalTime' column that can be merged on
            if 'SignalTime' not in raw_signals.columns: raw_signals['SignalTime'] = raw_signals.index
            merged = pd.merge(raw_signals.reset_index(drop=True), temp_predictions_df, on='SignalTime', how='left')
            filtered_signals = merged[merged['AI_Filter_Pass'] == 1].drop(columns=['AI_Filter_Pass'])


        logger.info(f"AIModelService: Filtered {len(raw_signals) - len(filtered_signals)} signals. {len(filtered_signals)} signals remain.")
        return filtered_signals.reset_index() # Ensure SignalTime is a column if it was index

    def _prepare_features_for_regime_detection(self, price_data: pd.DataFrame) -> pd.DataFrame:
        """
        Prepares features from price data for the regime detection model.
        Features could include volatility measures (e.g., ATR), trend indicators (e.g., ADX), etc.

        Args:
            price_data (pd.DataFrame): OHLCV data.

        Returns:
            pd.DataFrame: DataFrame of features for regime detection.
        """
        if price_data.empty:
            return pd.DataFrame()
        
        features_df = pd.DataFrame(index=price_data.index)
        
        # Example Features:
        # 1. ATR (Average True Range) for volatility
        # Calculation of ATR:
        if len(price_data) > 14: # Need enough data for ATR
            high_low = price_data['High'] - price_data['Low']
            high_close = np.abs(price_data['High'] - price_data['Close'].shift())
            low_close = np.abs(price_data['Low'] - price_data['Close'].shift())
            tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
            features_df['ATR_14'] = tr.rolling(window=14).mean()
        else:
            features_df['ATR_14'] = np.nan

        # 2. ADX (Average Directional Index) for trend strength (simplified placeholder)
        # A full ADX calculation is more involved.
        # if len(price_data) > 28: # Need enough data for ADX
        #     # Placeholder for ADX - replace with actual calculation
        #     features_df['ADX_14'] = price_data['Close'].rolling(window=14).std() # Simplified proxy
        # else:
        #     features_df['ADX_14'] = np.nan
            
        # Fill NaNs that arise from rolling calculations
        features_df.fillna(method='bfill', inplace=True) # Backfill first to handle start
        features_df.fillna(method='ffill', inplace=True) # Then forward fill
        features_df.fillna(0, inplace=True) # Fill any remaining with 0

        return features_df

    def train_regime_detection_model(self, features: pd.DataFrame, labels: pd.Series):
        """
        Placeholder for training a regime detection model.
        Labels would be (e.g., 0 for Ranging, 1 for Trending, 2 for Volatile).
        This would typically be a multi-class classification or clustering problem.
        """
        logger.info("AIModelService: Training regime detection model (placeholder)...")
        if features.empty or labels.empty:
            logger.error("AIModelService: Features or labels are empty. Cannot train regime model.")
            return
            
        # Example: Using RandomForest for multi-class classification
        # self.regime_detection_model = RandomForestClassifier(n_estimators=100, random_state=42)
        # X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)
        # self.regime_detection_model.fit(X_train, y_train) # Assuming features are already scaled if needed
        logger.info("AIModelService: Regime detection model trained (placeholder).")
        # Save model: joblib.dump(self.regime_detection_model, 'regime_detection_model.pkl')


    def detect_regime(self, price_data_for_current_moment: pd.DataFrame) -> str:
        """
        Detects the current market regime using the trained model.
        
        Args:
            price_data_for_current_moment (pd.DataFrame): Price data up to the point for which regime is to be detected.
                                                          The model would typically use the latest row of features.

        Returns:
            str: Detected regime (e.g., "Trending", "Ranging", "Volatile").
        """
        if self.regime_detection_model is None:
            logger.warning("AIModelService: Regime detection model not trained/loaded. Returning default.")
            return "Unknown" # Default regime
        if price_data_for_current_moment.empty:
            logger.warning("AIModelService: Empty price data for regime detection.")
            return "Unknown"

        features = self._prepare_features_for_regime_detection(price_data_for_current_moment)
        if features.empty:
            logger.warning("AIModelService: No features for regime detection. Returning default.")
            return "Unknown"
            
        # Use the latest set of features for prediction
        latest_features = features.iloc[-1:].copy() 
        
        # Ensure latest_features columns match training data (critical)
        # latest_features_scaled = self.regime_scaler.transform(latest_features) # If scaler was used
        
        try:
            # regime_code = self.regime_detection_model.predict(latest_features_scaled) # if scaled
            regime_code = self.regime_detection_model.predict(latest_features) # if not scaled or scaler part of pipeline
            regime_map = {0: "Ranging", 1: "Trending", 2: "Volatile"} # Example map
            return regime_map.get(regime_code[0], "Unknown")
        except Exception as e:
            logger.error(f"AIModelService: Error during regime prediction: {e}", exc_info=True)
            return "Unknown"

# Example of how to use (primarily for testing or direct script execution):
if __name__ == "__main__":
    logger.info("AIModelService script executed directly.")
    ai_service = AIModelService()

    # Create dummy data for testing
    dummy_dates = pd.to_datetime([f'2023-01-01 {i:02d}:00:00' for i in range(100)])
    dummy_price_data = pd.DataFrame({
        'Open': np.random.rand(100) * 10 + 100,
        'High': np.random.rand(100) * 2 + 100,
        'Low': np.random.rand(100) * -2 + 100,
        'Close': np.random.rand(100) * 10 + 100,
        'Volume': np.random.randint(1000, 5000, 100)
    }, index=dummy_dates)
    dummy_price_data['High'] = dummy_price_data[['Open', 'Close']].max(axis=1) + np.random.rand(100)
    dummy_price_data['Low'] = dummy_price_data[['Open', 'Close']].min(axis=1) - np.random.rand(100)
    
    # Localize to NY
    dummy_price_data = dummy_price_data.tz_localize("America/New_York")


    dummy_signals = pd.DataFrame({
        'SignalTime': dummy_dates[::10], # Signals every 10 bars
        'EntryPrice': dummy_price_data['Close'].iloc[::10].values,
        'SignalType': ['Long', 'Short'] * (len(dummy_dates[::10]) // 2 + 1) [:len(dummy_dates[::10])],
        'SL': dummy_price_data['Close'].iloc[::10].values - 1,
        'TP': dummy_price_data['Close'].iloc[::10].values + 2,
    }).set_index('SignalTime')

    # Test feature preparation
    logger.info("\nTesting feature preparation for signal filtering...")
    features_sig = ai_service._prepare_features_for_signal_filtering(dummy_price_data, dummy_signals)
    if not features_sig.empty:
        logger.info(f"Signal features prepared (first 5 rows):\n{features_sig.head()}")
    else:
        logger.info("Signal features DataFrame is empty.")

    # Test training (placeholder)
    # Create dummy labels for training signal filter
    if not features_sig.empty:
        dummy_labels_sig = pd.Series(np.random.randint(0, 2, size=len(features_sig)), index=features_sig.index)
        logger.info("\nTesting signal filter model training...")
        ai_service.train_signal_filter_model(features_sig, dummy_labels_sig)
        
        # Test signal filtering
        if ai_service.signal_filter_model:
            logger.info("\nTesting signal filtering...")
            # Re-index dummy_signals if it was modified or use the original one
            dummy_signals_for_filter = pd.DataFrame({
                'SignalTime': dummy_dates[::10], 
                'EntryPrice': dummy_price_data['Close'].iloc[::10].values,
                'SignalType': ['Long', 'Short'] * (len(dummy_dates[::10]) // 2 + 1) [:len(dummy_dates[::10])],
            }).set_index('SignalTime') # Ensure it's indexed for the filter function's current logic

            filtered_sigs = ai_service.filter_signals(dummy_price_data, dummy_signals_for_filter.reset_index()) # Pass with SignalTime as column
            logger.info(f"Filtered signals (first 5 rows):\n{filtered_sigs.head()}")
            logger.info(f"Original signals: {len(dummy_signals_for_filter)}, Filtered signals: {len(filtered_sigs)}")

    # Test feature preparation for regime detection
    logger.info("\nTesting feature preparation for regime detection...")
    features_regime = ai_service._prepare_features_for_regime_detection(dummy_price_data)
    if not features_regime.empty:
        logger.info(f"Regime features prepared (first 5 rows):\n{features_regime.head()}")
    else:
        logger.info("Regime features DataFrame is empty.")

    # Test regime detection training (placeholder)
    # if not features_regime.empty:
    #     dummy_labels_regime = pd.Series(np.random.randint(0, 3, size=len(features_regime)), index=features_regime.index)
    #     logger.info("\nTesting regime detection model training...")
    #     ai_service.train_regime_detection_model(features_regime, dummy_labels_regime)
        
        # Test regime detection
        # if ai_service.regime_detection_model:
        #     logger.info("\nTesting regime detection...")
        #     detected_reg = ai_service.detect_regime(dummy_price_data.iloc[:50]) # Use a slice of data
        #     logger.info(f"Detected regime for current moment (using first 50 bars): {detected_reg}")
